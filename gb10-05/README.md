# Lesson 5: "Vibe Coding" & Real-Time IDE Integration

## What is "Vibe Coding"?

Vibe coding is the transition from manually typing every line to curating code generated by the AI. On the GB10, this is superior to cloud-based tools (like Copilot) for two reasons:

- **Zero Data Leakage:** Your proprietary code never leaves the GB10.
- **Large Context Awareness:** You can feed the AI your entire local folder structure, which the GB10 can process instantly thanks to the Grace-Blackwell high-speed interconnect.

## Connect to your GB10 using the Remote SSH Extension

Setup the connection in VSCode
1. Click the Remote Explorer icon in the left bar. Hover over to see the names.
2. Hover over the SSH section and click the + icon
3. A box will appear at the top center of the window. Enter `ssh <username>@<gb10-ip>`
4. It should log you in automatically using passwordless SSH key authentication. If it prompts you for a password, something it's setup properly. You can enter your password but you'll have to do it everytime.

### Top open weights coding models

YMMV on these models but they are worth trying out. There are many others and it's worth trying them out. 

To add these to Ollama in your terminal, run:

```bash
docker exec -it ollama ollama pull qwen3-coder-next
docker exec -it ollama ollama pull qwen2.5-coder:32b
docker exec -it ollama ollama pull gpt-oss:120b
docker exec -it ollama ollama pull llama3.3:70b
```

unsloth has a lot of GGUF based releases of coding models https://huggingface.co/unsloth/models
```bash
docker exec -it ollama ollama pull hf.co/unsloth/gemma-3-27b-it-GGUF


hf download unsloth/MiniMax-M2.1-GGUF \
  --include "*UD-IQ3_XXS*" \
  --local-dir ~/gb10/models/MiniMax-M2.1-GGUF_Q8_0

hf download unsloth/Devstral-2-123B-Instruct-2512-GGUF \
  --include "*UD-Q4_K_XL*" \
  --local-dir ~/gb10/models/Devstral-2-123B-Instruct-2512-GGUF

hf download unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF \
  --include "*Qwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL.gguf" \
  --include "*UD-Q8_K_XL*" \
  --local-dir ~/gb10/models/Qwen3-Next-80B-A3B-Instruct-GGUF


```

## Hands-on Lab: Configuring Cline

1. Go to the Extensions view (`Ctrl + Shift + X`) and install **Cline**.
2. Click the Cline icon on the left bar and drag it to the right Chat panel if you'd prefer.
3. Click the gear icon at the top of the Cline sidebar. Select API Configuration
```
API Provider: ollama
Use custom base URL: http://192.168.0.30:11434
Model: Select a model
```
4. Follow a similar process for **Roo Code** if you want to try that extension as well. Compare them and see which you prefer. 

## Review of Extensions

None of the VSCode extensions are even close to Github Copilot but that was kind of expected. I think with time these open source extensions will improve.

Ranking of current VSCode extensions:

1. Github Copilot: This is my personal favorite. I prefer the Chat panel in VSCode. It's very feature rich and well designed. However, Ollama integration doesn't work, the models are populated but chats fail. There is a free tier for Cloud models. The subscription is $100/year and well worth it if you code a lot.
2. Claude Code: Terminal focused but has a Chat panel that seems to work. The Chat panel is less feature rich compared to Github Copilot. Uses a fixed Ollama model. 
3. Roo Code: Works okay, bare bones chat and editing capabilities. This is a fork of Cline with more features. Uses a fixed Ollama model. 
4. Cline: Works okay but limited features
5. Continue.dev: Lots of mention of this on the Internets but I couldn't get it to work properly with any models besides Llama3. None of the Ollama models would work with tool calling to edit files.

These are separate IDEs that have better support for AI coding assistants:

1. [Zed](https://zed.dev) is a local-first code editor with AI built-in. This is actually really good and works well with Ollama. Has a model picker to change what Ollama model you want to use.
2. [Kilo Code](https://kilo.ai/) Haven't tried this one but it seems to have good reviews.
3. [Windsurf](https://windsurf.com) is another AI coding assistant that can be self-hosted. 
4. SublimeText through the OllamaSublime or Yollama packages
5. NeoVim for you vim psychopaths

## Hands-on Lab: Vibe Coding Techniques

Now that it's set up, practice the three core "Vibes":

- **The Ghost Write (Tab):** Create a new `test.py` file. Start typing a function like `def calculate_sales_tax(amount):`. Wait 500ms. The GB10 will suggest the entire function in gray text. Press Tab to accept.
- **The Highlight (Cmd+L / Ctrl+L):** Highlight a block of code and press `Ctrl+L`. Ask: "Refactor this to be more efficient using a list comprehension."
- **The Intent (Cmd+I / Ctrl+I):** Press `Ctrl+I` to open the "Edit" box. Type: "Add a docstring to every function in this file." Watch as the GB10 edits the file in real-time.

---

**Task:** Create a new React or HTML component without "writing" any logic manually.

1. Create a blank file `Dashboard.html`.
2. Use `Ctrl+I` to say: "Build me a sales dashboard with a dark theme, a header, and a table showing 5 dummy products using Tailwind CSS."
3. Use `Ctrl+L` on the resulting table to say: "Add a 'Status' column that randomly displays 'Paid' or 'Pending' with green/yellow badges."

**Goal:** See how far you can get by only describing the "vibe" of the UI.

---

## What is Claude Code?

Unlike a standard Chatbot that just gives you code snippets, an Agent has "Tools."

- **File I/O:** Claude Code can read your entire codebase to understand context.
- **Terminal Access:** It can run `npm test`, `python script.py`, or `git commit`.
- **The Loop:** It thinks â†’ Proposes a change â†’ Runs a test â†’ If it fails, it tries again.

Why the GB10? Agentic loops are "token heavy." They send massive system prompts and file contexts (often 15k+ tokens). The GB10â€™s 128GB of VRAM ensures you can maintain these large context windows without the AI "forgetting" the beginning of the task.

Since January 2026, Ollama natively supports the Anthropic Messages API, making this setup much simpler than it used to be.

## Hands-on Lab: Setup & Integration

### Install Claude Code
Run the native installer on your GB10.

```bash
curl -fsSL https://claude.ai/install.sh | bash
```
Wait for that to finish then run
```bash
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc && source ~/.bashrc
```

Verify with `claude --version`.

### Configure the Local Bridge
We need to tell Claude Code to stop looking for Anthropic's servers and look at your local Ollama port (11434) instead.

Add these variables to your `~/.bashrc`:

```bash
export ANTHROPIC_BASE_URL="http://localhost:11434"
export ANTHROPIC_AUTH_TOKEN="ollama"
export CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1
```

Reload your shell: `source ~/.bashrc`.

### Selecting the Right Agentic Model
Not all models are good at "Tool Use." For the best results I recommend, which are specifically trained for agentic loops.

- `qwen3-coder-next`

Pull the model in Ollama:

```bash
docker exec -it ollama ollama pull qwen3-coder-next
```
3. Launching the Agent
Navigate to any code project directory on your GB10 and launch:

```bash
claude --model qwen3-coder-next
```

## Setup Claude Code in VSCode

Since your GB10 is already running Ollama and Claude Code, we just need to "plug in" VS Code.

### Extension install

1. Open VS Code on your laptop 
2. Select the Extensions tab on the left or hit Ctrl + Shift + P and search for Install Extension
3. Search for Remote - SSH and install
4. Search for Claude Code and install
6. Hit Ctrl + Shift + P and search for User Settings then select Preferences: Open User Settings
7. Go to Extensions > Claude Code and select
```
- Disable Login Prompt
- Select Model: qwen3-coder-next
- Use Terminal
```

## Using Claude Code

1. Select the Claude Code icon (orange sprite) in the top right
2. This will open the build-in VSCode Terminal and launch Claude Code in Terminal Mode. As of this writing it seems the extensions chat interface is broke when using a local LLM. We'll expore other options in future lessons.
3. Ensure the `qwen3-coder-next`is displayed in the top right. If not use `/model qwen3-coder-next` to select the model

### Useful Commands inside Claude Code

`/stats`: Shows you how many tokens you've used in the session.

`/compact`: Clears the "memory" of the conversation to save VRAM while keeping the current file context.

`/model`: Changes or selects a model to use

`/exit`: Exit session, you can just type `exit` as well

ðŸŒŸ Lesson 5 Challenge: Editing and creating local files

Prompt: Create a test directory in the current path and create 3 text files. Fill the files with loren ipsum text

Pro Tip: If Claude Code feels "slow" to start, itâ€™s usually because it is reading your .git history or large node_modules. Create a .claudeignore file in your project root to exclude those folders, just like a .gitignore.
