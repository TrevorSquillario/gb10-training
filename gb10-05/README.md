# Lesson 5: "Vibe Coding" & Real-Time IDE Integration

## What is "Vibe Coding"?

Vibe coding is the transition from manually typing every line to curating code generated by the AI. On the GB10, this is superior to cloud-based tools (like Copilot) for two reasons:

- **Zero Data Leakage:** Your proprietary code never leaves the GB10.
- **Large Context Awareness:** You can feed the AI your entire local folder structure, which the GB10 can process instantly thanks to the Grace-Blackwell high-speed interconnect.

## Connect to your GB10 using the Remote SSH Extension

Setup the connection in VSCode
1. Click the Remote Explorer icon in the left bar. Hover over to see the names.
2. Hover over the SSH section and click the + icon
3. A box will appear at the top center of the window. Enter `ssh <username>@<gb10-ip>`
4. It should log you in automatically using passwordless SSH key authentication. If it prompts you for a password, something isn't setup properly. Refer to `gb10-01`. You can enter your password but you'll have to do it everytime.

## Hands-on Lab: Github Copilot Chat

1. Go to the Extensions view (`Ctrl + Shift + X`) and install **Github Copilot Chat**.
2. Select View > Chat if the chat window doesn't show up on the right pane.
3. Go to the Accounts button in the bottom left above the Settings icon. Sign-in using a Github account. This will give you access to the free tier which provides 50 requests Premium Requests per month (all requests are treated as Premium with the free tier). With the Github Copilot Pro membership ($100/yr) you get unlimited access to the basic models like GPT-4o and GPT5-mini as well as additional Premium Requests for models like Claude Sonnet, Codex, etc.

## Hands-on Lab: Auto-Completion

- **The Ghost Write (Tab):** Create a new `test.py` file. Start typing a function like `def calculate_sales_tax(amount):`. Wait 500ms. The GB10 will suggest the entire function in gray text. Press Tab to accept. 
- **The Highlight:** Highlight a block of code, press `Ctrl+I` or go to your Chat. Ask: "Refactor this to accept a sales_tax parameter."

---

## Hands-on Lab: Using Local Models with Ollama

Ollama integration doesn't work with Github Copilot, the models are populated from Ollama but chats fail. At least with the models I tested with. 

### Top open weights coding models

YMMV on these models but they are worth trying out. There are many others but some do a better job at tool calling to edit files. Honestly they're not that good compared to the ones in Github Copilot. The models need to be tightly integrated with the IDE.

To add these to Ollama in your terminal, run:

```bash
docker exec -it ollama ollama pull qwen3-coder-next
docker exec -it ollama ollama pull qwen2.5-coder:32b
docker exec -it ollama ollama pull gpt-oss:120b
docker exec -it ollama ollama pull llama3.3:70b
```

unsloth has a lot of GGUF based releases of coding models https://huggingface.co/unsloth/models
```bash
docker exec -it ollama ollama pull hf.co/unsloth/gemma-3-27b-it-GGUF


hf download unsloth/MiniMax-M2.1-GGUF \
  --include "*UD-IQ3_XXS*" \
  --local-dir ~/gb10/models/MiniMax-M2.1-GGUF_Q8_0

hf download unsloth/Devstral-2-123B-Instruct-2512-GGUF \
  --include "*UD-Q4_K_XL*" \
  --local-dir ~/gb10/models/Devstral-2-123B-Instruct-2512-GGUF

hf download unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF \
  --include "*Qwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL.gguf" \
  --include "*UD-Q8_K_XL*" \
  --local-dir ~/gb10/models/Qwen3-Next-80B-A3B-Instruct-GGUF

hf download unsloth/Qwen3-Coder-Next-FP8-Dynamic  --local-dir ~/gb10/models/Qwen3-Coder-Next-FP8-Dynamic

```

## Review of Extensions

None of the VSCode extensions are even close to Github Copilot but that was kind of expected. I think with time these open source extensions will improve.

Ranking of current VSCode extensions:

1. Github Copilot: This is my personal favorite. I prefer the Chat panel in VSCode. It's very feature rich and well designed. However, Ollama integration doesn't work, the models are populated but chats fail. There is a free tier for Cloud models. The subscription is $100/year and well worth it if you code a lot.
2. Claude Code: Terminal focused but has a Chat panel that seems to work. The Chat panel is less feature rich compared to Github Copilot. Uses a fixed Ollama model. 
3. Roo Code: Works okay, bare bones chat and editing capabilities. This is a fork of Cline with more features. Uses a fixed Ollama model. 
4. Cline: Works okay but limited features
5. Continue.dev: Lots of mention of this on the Internets but I couldn't get it to work properly with any models besides Llama3. None of the Ollama models I tested would work with tool calling to edit files.

These are separate IDEs that have better support for AI coding assistants:

- [Goose](https://block.github.io/goose) This is an interesting project from the former CEO of Twitter. Similar to Claude Code but all open source. 
- [Zed](https://zed.dev) is a local-first code editor with AI built-in. This is actually really good and works well with Ollama. Has a model picker to change what Ollama model you want to use.
- [Kilo Code](https://kilo.ai/) Haven't tried this one but it seems to have good reviews.
- [Windsurf](https://windsurf.com) is another AI coding assistant that can be self-hosted. 
- SublimeText through the OllamaSublime or Yollama packages
- NeoVim for you vim psychopaths

### Configuring Cline or Roo Code

1. Go to the Extensions view (`Ctrl + Shift + X`) and install **Cline** or **Roo Code**.
2. Click the Cline icon on the left bar and drag it to the right Chat panel if you'd prefer.
3. Click the gear icon at the top of the Cline sidebar. Select API Configuration
```
API Provider: ollama
Use custom base URL: http://<gb10-ip>:11434
API Key: ollama
Model: Select a model
```
4. Follow a similar process for **Roo Code** if you want to try that extension as well. Compare them and see which you prefer. 

**Task:** Create a new React or HTML component without "writing" any logic manually.

1. Create a blank file `Dashboard.html`.
2. With the file selected ask: "Build me a sales dashboard with a dark theme, a header, and a table showing 5 dummy products using Tailwind CSS."
3. Then ask: "Add a 'Status' column that randomly displays 'Paid' or 'Pending' with green/yellow badges."

**Goal:** See how far you can get by only describing the "vibe" of the UI.

---

## What is Claude Code?

Unlike a standard Chatbot that just gives you code snippets, an Agent has "Tools."

- **File I/O:** Claude Code can read your entire codebase to understand context.
- **Terminal Access:** It can run `npm test`, `python script.py`, or `git commit`.
- **The Loop:** It thinks â†’ Proposes a change â†’ Runs a test â†’ If it fails, it tries again.

Why the GB10? Agentic loops are "token heavy." They send massive system prompts and file contexts (often 15k+ tokens). The GB10â€™s 128GB of VRAM ensures you can maintain these large context windows without the AI "forgetting" the beginning of the task.

Since January 2026, Ollama natively supports the Anthropic Messages API, making this setup much simpler than it used to be.

## Hands-on Lab: Setup & Integration

### Install Claude Code
Run the native installer on your GB10.

```bash
curl -fsSL https://claude.ai/install.sh | bash
```
Wait for that to finish then run
```bash
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc && source ~/.bashrc
```

Verify with `claude --version`.

### Configure the Local Bridge
We need to tell Claude Code to stop looking for Anthropic's servers and look at your local Ollama port (11434) instead.

Add these variables to your `~/.bashrc`:

```bash
export ANTHROPIC_BASE_URL="http://localhost:11434"
export ANTHROPIC_AUTH_TOKEN="ollama"
export CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1
```

Reload your shell: `source ~/.bashrc`.

### Selecting the Right Agentic Model
Not all models are good at "Tool Use." For the best results I recommend, which are specifically trained for agentic loops.

- `qwen3-coder-next`

Pull the model in Ollama:

```bash
docker exec -it ollama ollama pull qwen3-coder-next
```
3. Launching the Agent

The `claude` command is now accessible via the terminal and you can use it from there. 

Navigate to any code project directory on your GB10 and launch:

```bash
claude --model qwen3-coder-next
```

## Setup Claude Code in VSCode

Since your GB10 is already running Ollama and Claude Code, we just need to "plug in" VS Code. 

### Extension install

1. Open VS Code on your laptop 
2. Select the Extensions tab on the left, hit `Ctrl + Shift + X` or hit Ctrl + Shift + P and search for Install Extension
3. Search for Claude Code and install
4. Hit Ctrl + Shift + P and search for User Settings then select Preferences: Open User Settings
5. Go to Extensions > Claude Code and select
```
- Disable Login Prompt
- Select Model: qwen3-coder-next
- Use Terminal
```

## Using Claude Code in VSCode

1. Select the Claude Code icon (orange sprite) in the top right
2. This will open the build-in VSCode Terminal and launch Claude Code in Terminal Mode. As of this writing it seems the extensions chat interface is broke when using a local LLM. 
3. Ensure the `qwen3-coder-next`is displayed in the top right. If not use `/model qwen3-coder-next` to select the model

### Useful Commands inside Claude Code

`/stats`: Shows you how many tokens you've used in the session.

`/compact`: Clears the "memory" of the conversation to save VRAM while keeping the current file context.

`/model`: Changes or selects a model to use

`/exit`: Exit session, you can just type `exit` as well

ðŸŒŸ Lesson 5 Challenge: Editing and creating local files

Prompt: Create a test directory in the current path and create 3 text files. Fill the files with loren ipsum text

### Serve Your Coding Model with vLLM

This is generally not something you're going to need or use but it's here for reference. The comparison below focuses on parallel use (e.g. Claude Sub-Agents). However you will notice a faster TTFT (Time To First Token) compared to Ollama for sequential use (one prompt at a time). 

#### Throughput & Latency

Ollama (Parallel): Uses llama.cpp under the hood. It supports parallelism by creating multiple "slots" for the same model. However, it often handles them in a "round-robin" or limited batching fashion. If you send 4 subagent requests, Ollama might process them with some overhead, leading to a higher Time to First Token (TTFT) as the queue builds up.

vLLM (Parallel): Uses Continuous Batching. Instead of waiting for one request to finish, vLLM "injects" new subagent prompts into the current GPU calculation cycle dynamically. On a GB10, vLLM will likely be 3x to 5x faster in terms of total tokens per second when all subagents are firing at once.

#### Memory Management

Ollama: Uses traditional contiguous memory allocation. If you set OLLAMA_NUM_PARALLEL=4, it carves out 4 distinct blocks of VRAM for the KV cache. This can lead to "fragmentation" where you have enough total memory, but not enough in one continuous "chunk," causing OOM crashes.

vLLM: Uses PagedAttention. It treats GPU memory like a computer treats RAM (virtual memory). It breaks the KV cache into small "pages," meaning it uses almost 100% of the available VRAM with zero waste. For subagents with long chat histories, vLLM is far more stable

#### Setup

```bash
hf download Qwen/Qwen3-Coder-Next-FP8 \
  --local-dir ~/gb10/models/Qwen3-Coder-Next-FP8

cd gb10-05/vllm-coder
docker compose up -d
```

Edit your `~/.bashrc`:

```bash
export ANTHROPIC_BASE_URL="http://localhost:8000"
```

Launch claude
```bash
claude
```