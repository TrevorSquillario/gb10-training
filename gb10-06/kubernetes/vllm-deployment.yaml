---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-cluster
spec:
  replicas: 2 # Four instances for the class to see load balancing in action
  selector:
    matchLabels:
      app: vllm-demo # Changed to be specific
  template:
    metadata:
      labels:
        app: vllm-demo
    spec:
      runtimeClassName: nvidia
      containers:
      - name: vllm
        image: nvcr.io/nvidia/vllm:25.09-py3
        command: ["vllm", "serve", "/models/Qwen3-8B-NVFP4"]
        args:
        - "--quantization"
        - "modelopt"
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "8002"
        - "--gpu-memory-utilization"
        - "0.2" # CRITICAL: 4 pods * 0.2 = 80% total GPU usage
        - "--max-model-len"
        - "4096" # Limiting context length helps ensure 4 pods fit
        - "--trust-remote-code"
        ports:
        - containerPort: 8002
          name: http # This name is what the ServiceMonitor looks for
        resources:
          limits:
            nvidia.com/gpu: 1 # In time-slicing mode, this is a "logical" GPU
        volumeMounts:
        - name: model-volume
          mountPath: /models
        readinessProbe:
          httpGet:
            path: /health
            port: 8002
          initialDelaySeconds: 45
          periodSeconds: 10
      volumes:
      - name: model-volume
        hostPath:
          path: /mnt/models
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-lb-service
  labels:
    app: vllm-demo # <--- ADD THIS: The ServiceMonitor looks for this label here!
spec:
  type: NodePort
  selector:
    app: vllm-demo 
  ports:
    - name: http   # <--- ADD THIS: Matches the 'port: http' in your ServiceMonitor
      port: 8000 
      targetPort: 8002 
      nodePort: 30080