# Session 6: "Vibe Coding" & Real-Time IDE Integration

**Objective:** Master Vibe Codingâ€”a style of development where you stay in the "flow" by using natural language and real-time AI suggestions. Configure Continue.dev to use your GB10 as a local power source for autocomplete and chat.

## 1. What is "Vibe Coding"?

Vibe coding is the transition from manually typing every line to curating code generated by the AI. On the GB10, this is superior to cloud-based tools (like Copilot) for two reasons:

- **Zero Data Leakage:** Your proprietary code never leaves the GB10.
- **Large Context Awareness:** You can feed the AI your entire local folder structure, which the GB10 can process instantly thanks to the Grace-Blackwell high-speed interconnect.

## Hands-on Lab: Configuring Continue.dev

Since your GB10 is already running Ollama (from Session 4), we just need to "plug in" VS Code.

Go to the Extensions view (`Ctrl+Shift+X`) and install **Continue**.

### Pull the coding models

The `qwen3-coder-next` doesn't have proper tool calling support in Ollama so we'll use `qwen2.5-coder:32b` instead.

Autocomplete needs to be nearly instant. We use the 1.5B version of Qwen3 for this because it can reside permanently in a tiny corner of the GB10's 128GB memory.

In your terminal, run:

```bash

docker exec -it ollama ollama pull nomic-embed-text  
docker exec -it ollama ollama pull qwen2.5-coder:32b
docker exec -it ollama ollama pull qwen2.5-coder:1.5b
docker exec -it ollama ollama pull gpt-oss:120b
docker exec -it ollama ollama pull llama3.3:70b
```

### Configuring the config.json

1. Click the Continue icon on the left bar and drag it to the right Chat panel if you'd prefer.
2. Click the gear icon at the top of the Continue sidebar. 
3. Click the Configs icon and the gear next to Local Config. 
4. This yaml config below will use our local Ollama server and configure two models: one for Chat (Deep & Smart) and one for Autocomplete (Small & Fast). 

Replace the file contents with that of the `config.yaml` file in this lesson:

Adding your own models:
- The model name much match the output from `docker exec -it ollama ollama list` exactly
- Update the Local Config yaml with your new models

## Hands-on Lab: Configuring Cline

1. Go to the Extensions view (`Ctrl + Shift + X`) and install **Cline**.
2. Click the Cline icon on the left bar and drag it to the right Chat panel if you'd prefer.
3. Click the gear icon at the top of the Cline sidebar. Select API Configuration
```
API Provider: ollama
Use custom base URL: http://192.168.0.30:11434
Model: Select a model
```
4. Compare Cline and Continue.dev. 

## Review of Extensions

Neither VSCode extensions are even close to Github Copilot but that was kind of expected. I think with time these open source extensions will improve. Claude Code is a great option but it only supports terminal access which some may prefer. Some promising alternatives to checkout:

- Zed is a local-first code editor with AI built-in: https://zed.dev/
- Codeium is another AI coding assistant that can be self-hosted: https://codeium.com/

## Hands-on Lab: Vibe Coding Techniques

Now that it's set up, practice the three core "Vibes":

- **The Ghost Write (Tab):** Create a new `test.py` file. Start typing a function like `def calculate_sales_tax(amount):`. Wait 500ms. The GB10 will suggest the entire function in gray text. Press Tab to accept.
- **The Highlight (Cmd+L / Ctrl+L):** Highlight a block of code and press `Ctrl+L`. Ask: "Refactor this to be more efficient using a list comprehension."
- **The Intent (Cmd+I / Ctrl+I):** Press `Ctrl+I` to open the "Edit" box. Type: "Add a docstring to every function in this file." Watch as the GB10 edits the file in real-time.

---

ðŸŒŸ **Session 6 Challenge: The "No-Keyboard" Component**

**Task:** Create a new React or HTML component without "writing" any logic manually.

1. Create a blank file `Dashboard.html`.
2. Use `Ctrl+I` to say: "Build me a sales dashboard with a dark theme, a header, and a table showing 5 dummy products using Tailwind CSS."
3. Use `Ctrl+L` on the resulting table to say: "Add a 'Status' column that randomly displays 'Paid' or 'Pending' with green/yellow badges."

**Goal:** See how far you can get by only describing the "vibe" of the UI.

---

## Resources for Session 6

- Playbook: Vibe Coding in VS Code
- Extension Site: Continue.dev Documentation

> **Pro Tip:** If the autocomplete feels sluggish, it's usually because the 1GbE network latency (between your laptop and the GB10) is higher than the AI's generation time. For the snappiest experience, ensure you are on a wired connection rather than Wi-Fi.